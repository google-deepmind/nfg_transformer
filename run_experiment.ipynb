{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "executionInfo": {
          "elapsed": 366,
          "status": "ok",
          "timestamp": 1705676857474,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 0
        },
        "id": "QDsLVIbQPdPp"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade https://github.com/google-deepmind/nfg_transformer.git\n",
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "executionInfo": {
          "elapsed": 192,
          "status": "ok",
          "timestamp": 1705676858556,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 0
        },
        "id": "JpNVLYMQQxAC"
      },
      "outputs": [],
      "source": [
        "# @title Implementation for supervised learning experiments for the NfgTransfomer.\n",
        "\n",
        "import enum\n",
        "import re\n",
        "from typing import Mapping\n",
        "from typing import Sequence\n",
        "\n",
        "from absl import logging\n",
        "import haiku as hk\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "\n",
        "from nfg_transformer import equilibria\n",
        "from nfg_transformer import games\n",
        "from nfg_transformer import network\n",
        "\n",
        "\n",
        "\n",
        "######\n",
        "# OBJECTIVES\n",
        "######\n",
        "\n",
        "\n",
        "class Objective(enum.Enum):\n",
        "  RECONSTRUCTION = enum.auto()\n",
        "  NE = enum.auto()\n",
        "  MAX_DEVIATION_GAIN = enum.auto()\n",
        "\n",
        "\n",
        "######\n",
        "# MODEL \u0026 LEARNING RULES\n",
        "######\n",
        "\n",
        "\n",
        "def _joint_mask_to_action_masks(mask: jnp.ndarray) -\u003e Sequence[jnp.ndarray]:\n",
        "  action_masks = []\n",
        "  for p in range(mask.ndim):\n",
        "    not_p = [q for q in range(mask.ndim) if q != p]\n",
        "    action_masks.append(jnp.any(mask, axis=not_p))\n",
        "  return action_masks  # [N, T]\n",
        "\n",
        "\n",
        "def make_model(\n",
        "    objective: Objective,\n",
        "    optim: optax.GradientTransformation,\n",
        "    num_heads: int,\n",
        "    num_action_channels: int,\n",
        "    num_qkv_channels: int,\n",
        "    num_blocks: int,\n",
        "    num_self_attend_per_block: int,\n",
        "):\n",
        "  \"\"\"Returns neural network model functions.\n",
        "\n",
        "  Args:\n",
        "    objective: the objective that the model is optimised for.\n",
        "    optim: the optimiser used.\n",
        "    num_heads: the number of attention heads.\n",
        "    num_action_channels: the action embedding dimension.\n",
        "    num_qkv_channels: the internal dimensionality of the query, key, value\n",
        "      vectors.\n",
        "    num_blocks: the number of NfgTransformerBlock.\n",
        "    num_self_attend_per_block: the number of action-to-action self-attention\n",
        "      layers in each NfgTransformerBlock.\n",
        "\n",
        "  Returns:\n",
        "    initial_params: a function that returns intialised parameters for the model.\n",
        "    update: a function that returns updated parameters given the current\n",
        "      parameters and data.\n",
        "    evaluate: a function that run forward inference and loss computation for\n",
        "      the (masked) input payoffs.\n",
        "  \"\"\"\n",
        "\n",
        "  def _forward(payoffs, masks):\n",
        "    \"\"\"Encodes payoffs as action embeddings and decodes for an objective.\n",
        "\n",
        "    Args:\n",
        "      payoffs: a tensor of shape [N, T1, ..., TN] representing payoffs.\n",
        "      masks: a tensor of shape [N, T1, ..., TN] indicating if each joint action\n",
        "        is masked.\n",
        "\n",
        "    Returns:\n",
        "      outputs: a tree of arrays representing the decoded outputs.\n",
        "    \"\"\"\n",
        "    # Encodes masked payoffs as action embeddings.\n",
        "    actions = network.NfgTransformer(\n",
        "        num_heads=num_heads,\n",
        "        num_action_channels=num_action_channels,\n",
        "        num_qkv_channels=num_qkv_channels,\n",
        "        num_blocks=num_blocks,\n",
        "        num_self_attend_per_block=num_self_attend_per_block,\n",
        "    )(payoffs, masks)\n",
        "\n",
        "    # Select an appropriate decoder architecture for an objective.\n",
        "    if objective == Objective.NE:\n",
        "      outputs = network.NfgPerAction(name=objective.name)(actions)\n",
        "    elif objective == Objective.MAX_DEVIATION_GAIN:\n",
        "      outputs = network.NfgPerJoint(name=objective.name)(actions)\n",
        "    elif objective == Objective.RECONSTRUCTION:\n",
        "      outputs = network.NfgPerPayoff(\n",
        "          num_heads=num_heads,\n",
        "          qk_channels=num_qkv_channels,\n",
        "          v_channels=num_qkv_channels,\n",
        "          name=objective.name,\n",
        "      )(actions)\n",
        "    else:\n",
        "      raise ValueError(f'Unrecognised objective ({objective}).')\n",
        "\n",
        "    return outputs\n",
        "\n",
        "  def _loss_fn(payoffs, masks):\n",
        "    \"\"\"Returns loss and extra statistics from payoffs.\"\"\"\n",
        "    outputs = _forward(payoffs, masks)\n",
        "    if objective == Objective.RECONSTRUCTION:\n",
        "      inpainting_loss = jnp.mean(jnp.square(payoffs - outputs) * (1 - masks))\n",
        "      loss = jnp.mean(jnp.square(payoffs - outputs))\n",
        "      extra = dict(pred=outputs, inpainting_loss=inpainting_loss)\n",
        "    elif objective == Objective.NE:\n",
        "      logits = outputs\n",
        "      action_mask = _joint_mask_to_action_masks(masks)\n",
        "      logits = [jnp.where(m, l, -jnp.inf) for m, l in zip(action_mask, logits)]\n",
        "      loss, extra = equilibria.nash_approx(payoffs, logits, joint_mask=masks)\n",
        "      extra = dict(\n",
        "          marginals=[jax.nn.softmax(l) for l in logits],\n",
        "          logits=logits,\n",
        "          action_mask=action_mask,\n",
        "          **extra,\n",
        "      )\n",
        "    elif objective == Objective.MAX_DEVIATION_GAIN:\n",
        "      loss, extra = equilibria.max_deviation_gain(payoffs, outputs)\n",
        "    else:\n",
        "      raise ValueError(f'Unrecognised objective ({objective}).')\n",
        "\n",
        "    extra = {'payoffs': payoffs, 'masks': masks, **extra}\n",
        "    return loss, extra\n",
        "\n",
        "  def initial_params(key, payoffs, masks):\n",
        "    return hk.transform(_loss_fn).init(key, payoffs, masks)\n",
        "\n",
        "  @jax.jit\n",
        "  def evaluate(params, key, payoffs, masks):\n",
        "    key, this_key = jax.random.split(key)\n",
        "    loss_fn = hk.transform(jax.vmap(_loss_fn))\n",
        "    loss, extra = loss_fn.apply(params, this_key, payoffs, masks)\n",
        "    extra = jax.tree_map(lambda arr: arr[0], extra)\n",
        "\n",
        "    return jnp.mean(loss), (key, extra)\n",
        "\n",
        "  @jax.jit\n",
        "  def update(params, key, opt_state, payoffs, masks):\n",
        "    (loss, (key, extra)), grads = jax.value_and_grad(evaluate, has_aux=True)(\n",
        "        params, key, payoffs, masks\n",
        "    )\n",
        "    extra['grad_norm'] = optax.global_norm(grads)\n",
        "    update, opt_state = optim.update(grads, opt_state, params=params)\n",
        "    extra['update_norm'] = optax.global_norm(update)\n",
        "    params = optax.apply_updates(params, update)\n",
        "    extra['params_norm'] = optax.global_norm(params)\n",
        "    return params, key, opt_state, (loss, extra)\n",
        "\n",
        "  return initial_params, update, evaluate\n",
        "\n",
        "\n",
        "######\n",
        "# OPTIMISER\n",
        "######\n",
        "\n",
        "\n",
        "def _generate_mask(\n",
        "    pattern_include: re.Pattern[str],\n",
        "    pattern_exclude: re.Pattern[str],\n",
        "    params,\n",
        "    parent_key: str = '',\n",
        "):\n",
        "  \"\"\"Get a weight mask based on parameter names.\"\"\"\n",
        "  processed = {}\n",
        "  for k, v in params.items():\n",
        "    path = parent_key + '/' + k if parent_key else k\n",
        "    if isinstance(v, Mapping):\n",
        "      processed[k] = _generate_mask(pattern_include, pattern_exclude, v, path)\n",
        "    else:\n",
        "      processed[k] = (\n",
        "          pattern_include.match(path) is not None\n",
        "          and pattern_exclude.match(path) is None\n",
        "      )\n",
        "  return processed\n",
        "\n",
        "\n",
        "_WEIGHT_DECAY_INCLUDE_PARAMS = '()'\n",
        "_WEIGHT_DECAY_EXCLUDE_PARAMS = '(.*/b$|.*/gamma$|.*layer_norm.*)'\n",
        "\n",
        "\n",
        "def _weight_decay_param_mask(params: hk.Params) -\u003e Mapping[str, bool]:\n",
        "  include_re = re.compile(_WEIGHT_DECAY_INCLUDE_PARAMS)\n",
        "  exclude_re = re.compile(_WEIGHT_DECAY_EXCLUDE_PARAMS)\n",
        "  mask = _generate_mask(include_re, exclude_re, params)\n",
        "  logging.info('Optimiser weight decay mask: %s', mask)\n",
        "  return mask\n",
        "\n",
        "\n",
        "@optax.inject_hyperparams\n",
        "def optimiser(\n",
        "    learning_rate: float,\n",
        ") -\u003e optax.GradientTransformation:\n",
        "  \"\"\"Returns optax.chain of optimiser transforms.\"\"\"\n",
        "  return optax.chain(\n",
        "      optax.adaptive_grad_clip(0.01),\n",
        "      optax.adamw(\n",
        "          learning_rate,\n",
        "          weight_decay=0.1,\n",
        "          mask=_weight_decay_param_mask,\n",
        "      ),\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "executionInfo": {
          "elapsed": 928300,
          "status": "ok",
          "timestamp": 1705676354428,
          "user": {
            "displayName": "Siqi Liu",
            "userId": "13169359758358957280"
          },
          "user_tz": 0
        },
        "id": "QEnZvdg5ldM7",
        "outputId": "b9d3d9ec-f97f-420b-8c45-799ce5ef6bf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 0: loss = 0.565171480178833\n",
            "Iteration 1000: loss = 0.3001517057418823\n",
            "Iteration 2000: loss = 0.24752219021320343\n",
            "Iteration 3000: loss = 0.18539714813232422\n",
            "Iteration 4000: loss = 0.1863342523574829\n",
            "Iteration 5000: loss = 0.1475866436958313\n",
            "Iteration 6000: loss = 0.13515670597553253\n",
            "Iteration 7000: loss = 0.13579054176807404\n",
            "Iteration 8000: loss = 0.07884744554758072\n",
            "Iteration 9000: loss = 0.06277988106012344\n",
            "Iteration 10000: loss = 0.0499388761818409\n",
            "Iteration 11000: loss = 0.03781219571828842\n",
            "Iteration 12000: loss = 0.027606459334492683\n",
            "Iteration 13000: loss = 0.038404859602451324\n",
            "Iteration 14000: loss = 0.03488107770681381\n",
            "Iteration 15000: loss = 0.026034243404865265\n",
            "Iteration 16000: loss = 0.028706500306725502\n",
            "Iteration 17000: loss = 0.02510036528110504\n",
            "Iteration 18000: loss = 0.021910671144723892\n",
            "Iteration 19000: loss = 0.014682113192975521\n"
          ]
        }
      ],
      "source": [
        "#@title Select a learning objective and optimise an instance of the NfgTransformer on n-player general-sum NFGs.\n",
        "\n",
        "max_num_updates = 20000  # @param {type:\"integer\"}\n",
        "batch_size = 32 # @param {type:\"integer\"}\n",
        "num_strategies = (16, 16)  # @param {type:\"raw\"}\n",
        "\n",
        "objective = Objective.NE # @param [\"Objective.NE\", \"Objective.MAX_DEVIATION_GAIN\", \"Objective.RECONSTRUCTION\"] {type:\"raw\"}\n",
        "\n",
        "if objective in (Objective.NE, Objective.MAX_DEVIATION_GAIN):\n",
        "  # Sample batches of payoff tensors from the L2-invariant game subspace.\n",
        "  generate_payoffs = games.generate_payoffs(\n",
        "      games.Game.L2_INVARIANT,\n",
        "      {},\n",
        "      num_strategies=num_strategies,\n",
        "      batch_size=batch_size,\n",
        "  )\n",
        "elif objective == Objective.RECONSTRUCTION:\n",
        "  # Sample batches of payoff tensors of empirical disc games for payoff\n",
        "  # prediction. Observing a random subset of the joint-actions.\n",
        "  generate_payoffs = games.generate_payoffs(\n",
        "      games.Game.EMPIRICAL_DISC_GAME,\n",
        "      {\"joint_action_keep_prob\": 0.5, \"latent_size\": 4},\n",
        "      num_strategies=num_strategies,\n",
        "      batch_size=batch_size,\n",
        "  )\n",
        "else:\n",
        "  raise ValueError(f\"Unknown objective: {objective}\")\n",
        "\n",
        "# Use an adam optimiser with annealed learning rate and weight decay.\n",
        "optim = optimiser(\n",
        "    optax.warmup_cosine_decay_schedule(\n",
        "        init_value=0.0,\n",
        "        peak_value=1e-4,\n",
        "        warmup_steps=1_000,\n",
        "        decay_steps=max_num_updates,\n",
        "        end_value=1e-6,\n",
        "        exponent=1.0,\n",
        "    )\n",
        ")\n",
        "\n",
        "# NfgTransformer(D=64, K=8, A=2)\n",
        "initial_params, update, evaluate = make_model(\n",
        "    objective=objective,\n",
        "    num_heads=8,\n",
        "    num_action_channels=64,\n",
        "    num_qkv_channels=64,\n",
        "    num_blocks=8,\n",
        "    num_self_attend_per_block=2,\n",
        "    optim=optim,\n",
        ")\n",
        "\n",
        "key = hk.PRNGSequence(42)\n",
        "params = initial_params(\n",
        "    next(key),\n",
        "    payoffs=jnp.zeros((len(num_strategies),) + num_strategies),\n",
        "    masks=jnp.ones(num_strategies),\n",
        ")\n",
        "opt_state = optim.init(params)\n",
        "\n",
        "key = next(key)\n",
        "\n",
        "losses = []\n",
        "for i in range(max_num_updates):\n",
        "  (payoffs, masks), key = generate_payoffs(key)\n",
        "  params, key, opt_state, (loss, _) = update(\n",
        "      params, key, opt_state, payoffs, masks\n",
        "  )\n",
        "  if (i % 1000 == 0):\n",
        "    print(f\"Iteration {i}: loss = {loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "executionInfo": {
          "elapsed": 6473,
          "status": "ok",
          "timestamp": 1705676361285,
          "user": {
            "displayName": "Siqi Liu",
            "userId": "13169359758358957280"
          },
          "user_tz": 0
        },
        "id": "UYYnYSD6zWXe",
        "outputId": "c713aa2f-54ef-427c-f90c-8b6d331e985d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 0.03225991129875183\n",
            "Payoffs:\n",
            " [[[-1.21  0.09 -0.97  0.19 -0.15  0.72 -0.08  0.1   0.88 -0.05 -0.07 -0.37  1.04 -0.56  0.41 -0.42]\n",
            "  [ 0.21  0.69  1.85  0.53  0.36 -0.88 -1.48  0.92  0.6   0.01 -1.11 -2.47 -0.57  1.28 -0.67  0.25]\n",
            "  [ 0.86 -0.39  0.96 -0.97 -1.1   1.08 -1.56 -0.05  0.44 -0.18 -0.48 -0.52 -0.39 -1.53 -0.66  0.55]\n",
            "  [ 1.59  0.2  -0.84 -1.05 -0.46 -0.68  0.37 -1.32  0.34  2.16  0.74  0.5  -0.44  0.87 -0.14  0.14]\n",
            "  [-0.25  1.13  1.45  0.35  1.85 -0.89  0.18 -0.12 -1.22  0.08 -1.7  -0.71  0.93 -1.04  0.59  0.75]\n",
            "  [ 0.67 -1.69  0.19 -1.43  1.41  0.08  0.35 -1.67 -0.48  0.02  0.41 -0.28 -1.31  1.52  0.59 -1.06]\n",
            "  [-2.12 -0.44  1.07  1.1  -1.53  0.67 -0.95  2.44 -0.37  0.99 -0.88  0.56  0.09 -0.5  -1.23 -1.39]\n",
            "  [-0.13  0.87 -1.17  0.29 -0.19 -0.12  0.79  4.02 -0.71 -0.12  0.91 -1.14 -0.35  1.96  0.25 -0.54]\n",
            "  [-0.24  0.44 -0.11 -1.01  0.82  2.02 -0.53 -0.9  -0.12  1.18 -0.17  0.28 -0.71  0.21  0.21 -0.69]\n",
            "  [ 0.01  0.65 -1.   -0.41  0.93  0.01  0.78 -0.69  0.3  -1.62 -1.04  0.01  1.14 -0.87 -0.94 -0.9 ]\n",
            "  [ 0.56 -1.   -0.72  2.09 -0.83 -1.4   1.57  1.17  0.25 -0.3   0.42  0.18  0.51  0.13  0.79 -0.38]\n",
            "  [-0.86  0.25  0.42  0.32  1.59 -0.9   0.7   1.07  1.02 -1.46  0.07  1.22 -1.42 -0.61  0.14  0.18]\n",
            "  [-2.28 -0.81  0.16  1.39 -0.84  0.67 -1.23 -2.18  0.53 -1.36 -1.29  0.3   0.11  0.13  0.74 -0.1 ]\n",
            "  [ 0.24 -0.38  0.62 -0.36 -0.45  0.01  1.02 -0.86 -1.2  -1.91  0.86 -1.86  0.04  0.42  0.01  1.1 ]\n",
            "  [ 1.06 -0.07 -1.2  -1.62 -1.44  0.2  -0.81 -0.58  0.77  0.87  2.59  1.36  1.36 -0.85  0.04  1.75]\n",
            "  [ 1.9   0.48 -0.71  0.58  0.03 -0.61  0.88 -1.34 -1.04  1.68  0.74  2.92 -0.01 -0.56 -0.14  0.79]]\n",
            "\n",
            " [[-0.02  0.   -0.5  -1.3   1.52  0.69 -0.03  1.57  0.94  0.12 -0.66 -0.03 -0.25 -1.38 -0.42 -0.25]\n",
            "  [-0.43 -0.67 -0.93  0.64 -0.4  -2.24 -1.35  0.26  0.44  1.84 -0.76  1.35  1.51 -0.27 -0.14  1.16]\n",
            "  [ 1.38 -0.2   1.35 -0.44  0.48  0.54 -0.75  0.55 -0.65 -0.92  0.63 -1.29  1.35 -0.33 -1.61 -0.08]\n",
            "  [ 1.12 -0.2  -0.2   0.18  0.65  0.98 -1.2  -0.97  0.7  -2.17  2.94 -1.32  0.28 -0.67  0.38 -0.48]\n",
            "  [-0.37  0.86  2.55  1.15  0.04 -0.53 -0.85 -0.77 -1.11  0.18 -0.31 -1.72  0.22  1.46 -0.28 -0.51]\n",
            "  [ 1.08 -0.07 -0.61  0.99  0.2   0.92 -1.64  0.74 -1.8  -1.41  0.96  0.05  0.97  0.27 -1.3   0.66]\n",
            "  [-0.25  0.34  0.35  0.79 -0.53 -0.2   1.53 -1.03 -2.23 -0.36 -0.08 -0.96  0.12  0.34  0.99  1.18]\n",
            "  [ 0.39  0.01  0.22  1.1  -0.54  0.99  0.03 -0.52 -0.3  -0.72  0.4  -0.13  1.4  -0.17 -1.91 -0.25]\n",
            "  [-1.   -0.69  0.34  1.3  -0.72 -0.89 -0.3   0.07  1.95  0.75 -0.52  0.15 -0.07  0.95  0.66 -1.98]\n",
            "  [ 1.47 -1.21 -1.75 -1.2  -1.48 -0.74  0.94  1.09  1.84  0.32 -2.51  0.14  1.38  2.16 -0.71  0.26]\n",
            "  [-1.48 -0.32  1.39  0.86 -1.29  0.9  -0.58  0.92 -0.64  1.14 -0.43 -0.78  0.75 -0.   -1.43  1.  ]\n",
            "  [-0.25  1.8   0.17  0.23 -1.43 -0.93 -0.23  0.43 -0.49 -0.33 -1.17  0.94  1.08  0.08 -1.45  1.53]\n",
            "  [ 1.11  1.5   0.45  0.36 -0.23  0.96  0.11 -0.86  0.5  -0.52  0.57 -1.69 -0.65  0.9  -2.66  0.14]\n",
            "  [ 0.72  0.54 -0.38 -0.42  0.4  -0.49 -0.65  0.44 -0.01  0.28  0.85 -1.3  -0.94 -2.08  0.91  2.12]\n",
            "  [-0.35 -0.91  0.8  -0.2   1.89 -2.33  0.85 -1.23 -1.35 -0.3   0.11  0.62  0.82  0.7   0.73  0.15]\n",
            "  [ 0.69 -0.05 -1.17  1.18 -0.29  0.14  0.18 -1.07  0.24 -0.77  2.2   0.44  0.49 -0.54 -1.2  -0.47]]]\n",
            "Masks:\n",
            " [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "===\n",
            "\n",
            "Model outputs:\n",
            "action_mask:\n",
            "[Array([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool), Array([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)]\n",
            "entropy:\n",
            "1.1779091358184814\n",
            "logits:\n",
            "[Array([-7.52,  4.71, -7.15, -7.56,  5.05, -7.35, -6.98, -7.53, -7.5 , -7.53, -7.58, -7.41, -7.49, -7.31, -7.58, -7.45], dtype=float32), Array([-7.46, -7.25,  5.27, -6.82, -7.21, -7.47, -7.45, -7.5 , -7.41, -6.72, -7.36, -7.24,  3.87, -6.94, -7.17, -7.28], dtype=float32)]\n",
            "marginals:\n",
            "[Array([0.  , 0.42, 0.  , 0.  , 0.58, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ], dtype=float32), Array([0. , 0. , 0.8, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.2, 0. , 0. , 0. ], dtype=float32)]\n",
            "total_deviation_gain:\n",
            "0.08339287340641022\n"
          ]
        }
      ],
      "source": [
        "# @title Randomly sample a batch of new payoffs and inspect the outputs.\n",
        "import numpy as np\n",
        "\n",
        "(payoffs, masks), key = generate_payoffs(key)\n",
        "\n",
        "loss, (key, extra) = evaluate(params, key, payoffs, masks)\n",
        "\n",
        "print(f\"Loss (average): {loss}\")\n",
        "np.set_printoptions(suppress=True, precision=2, linewidth=180)\n",
        "\n",
        "print(\"Payoffs:\\n\", extra.pop(\"payoffs\"))\n",
        "print(\"Masks:\\n\", extra.pop(\"masks\"))\n",
        "\n",
        "print(\"######\")\n",
        "print(\"# Model outputs:\")\n",
        "print(\"######\\n\")\n",
        "\n",
        "for k, v in extra.items():\n",
        "  print(f\"{k}:\\n{v}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/deepmind/dm_python:dm_notebook3",
        "kind": "private"
      },
      "provenance": [
        {
          "file_id": "17suRMgnPUIwpQG2ZBUHU9trAoRP1Jgxw",
          "timestamp": 1705676470310
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
